{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'c:\\Users\\HOME\\Downloads\\data.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
    "df_cleaned = df_cleaned[(df_cleaned['Quantity'] > 0) & (df_cleaned['UnitPrice'] > 0)]\n",
    "df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "df_cleaned['Sales'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicate rows\n",
    "duplicate_rows_count = df_cleaned.duplicated().sum()\n",
    "\n",
    "print(f\"The number of duplicate rows is: {duplicate_rows_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping duplicate values\n",
    "df_cleaned.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[['Quantity', 'UnitPrice']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description Countplot\n",
    "item_counts = df_cleaned['Description'].value_counts().sort_values(ascending=False).iloc[0:15]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.barplot(x=item_counts.index, y=item_counts.values, palette='viridis')  # Change the color palette to 'viridis'\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which items were bought more often?\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Description'].value_counts().nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'].value_counts().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock codes Count plot\n",
    "stock_counts = df_cleaned['StockCode'].value_counts().sort_values(ascending=False).iloc[0:15]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.barplot(x=stock_counts.index, y=stock_counts.values, palette='mako')  # Change the color palette to 'YlOrBr'\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which stock codes were used the most?\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['StockCode'].value_counts().nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the quantity sold for each product\n",
    "top_products = df_cleaned.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10).reset_index()\n",
    "\n",
    "# Create a bar chart with separate colors using Matplotlib and Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_products, x='Description', y='Quantity', palette='Set2')\n",
    "plt.title('Top 10 Products by Quantity Sold')\n",
    "plt.xlabel('Product Description')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoice with most number of items\n",
    "inv_counts = df_cleaned['InvoiceNo'].value_counts().sort_values(ascending=False).iloc[0:15]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.barplot(x=inv_counts.index, y=inv_counts.values, palette='crest')  \n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which invoices had the most items?\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Description and calculate total sales for each product\n",
    "product_sales = df_cleaned.groupby('Description')['Sales'].sum().sort_values(ascending=False).head(10).reset_index()\n",
    "\n",
    "# Bar chart with different colors for each bar using Matplotlib and Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=product_sales, x='Description', y='Sales', palette='Set1')\n",
    "plt.title('Top 10 Products by Sales')\n",
    "plt.xlabel('Product Description')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned['Country'] != \"United Kingdom\"]['Country'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Countries and calculate total sales for each Country(Excluding United Kingdom)\n",
    "product_sales_country = df_cleaned.groupby('Country')['Sales'].sum().sort_values(ascending=False).head(10).reset_index()\n",
    "\n",
    "# Bar chart \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=product_sales_country[product_sales_country['Country'] != \"United Kingdom\"], x='Country', y='Sales', palette='Set1')\n",
    "plt.title(\"Distribution of Sales over the top 10 countries by sales outside the UK\")\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Sales for United Kingdom is\",df_cleaned[df_cleaned[\"Country\"] == 'United Kingdom']['Sales'].sum().round(2))\n",
    "print(\"Total Sales for Other is\",df_cleaned[df_cleaned[\"Country\"] != 'United Kingdom']['Sales'].sum().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales trend\n",
    "df1 = df_cleaned.copy()\n",
    "df1['InvoiceDate'] = pd.to_datetime(df1['InvoiceDate'])\n",
    "df1['Month'] = df1['InvoiceDate'].dt.to_period('M')\n",
    "monthly_sales = df1.groupby('Month')['Quantity'].sum()\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_sales.plot(marker='o')\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for outlier removal based on IQR\n",
    "def remove_outliers(df, column_name):\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_outlier_free = df[(df[column_name] >= Q1 - 1.5 * IQR) & (df[column_name] <= Q3 + 1.5 * IQR)]\n",
    "    return df_outlier_free\n",
    "# Remove outliers for 'UnitPrice' and 'Quantity'\n",
    "df_outlier_free_unit_price = remove_outliers(df_cleaned, 'UnitPrice')\n",
    "df_cleaned = remove_outliers(df_outlier_free_unit_price, 'Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_outlier_free_unit_price['UnitPrice'], bins=10, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of UnitPrice')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_cleaned['Quantity'], bins=10, color='green', alpha=0.7)\n",
    "plt.title('Histogram of Quantity')\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Data by Date\n",
    "df_aggregated = df_cleaned.groupby(pd.Grouper(key='InvoiceDate', freq='D')).agg({'Sales': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic statistics\n",
    "print(\"Basic Statistics:\")\n",
    "print(df_aggregated['Sales'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_aggregated.isnull().sum())\n",
    "\n",
    "df_filtered = df_aggregated[df_aggregated['InvoiceDate'] > '2010-12-31']\n",
    "\n",
    "# Time series plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df_filtered['InvoiceDate'], df_filtered['Sales'])\n",
    "plt.title('Time Series Plot of Daily Sales')\n",
    "plt.xlabel('Invoice Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for seasonality and outliers\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(df_filtered['InvoiceDate'].dt.month, df_filtered['Sales'])\n",
    "plt.title('Monthly Sales Distribution')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "autocorrelation_plot(df_filtered['Sales'])\n",
    "plt.title('Autocorrelation Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df_filtered['Sales'], bins=30)\n",
    "plt.title('Histogram of Daily Sales')\n",
    "plt.xlabel('Total Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Moving Average\n",
    "df_filtered['Moving_Avg'] = df_filtered['Sales'].rolling(window=7).mean()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df_filtered['InvoiceDate'], df_filtered['Moving_Avg'])\n",
    "plt.title('7-Day Moving Average of Daily Sales')\n",
    "plt.xlabel('Invoice Date')\n",
    "plt.ylabel('Total Price (7-Day Avg)')\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Seasonal Decomposition\n",
    "result = seasonal_decompose(df_filtered['Sales'].fillna(0), period=30)  # Monthly seasonality\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "df_filtered['Year'] = df_filtered['InvoiceDate'].dt.year\n",
    "df_filtered['Month'] = df_filtered['InvoiceDate'].dt.month\n",
    "pivot_table = df_filtered.pivot_table(values='Sales', index='Month', columns='Year', aggfunc='sum')\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".1f\")\n",
    "plt.title('Monthly Sales Heatmap')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month')\n",
    "plt.show()\n",
    "\n",
    "from pandas.plotting import lag_plot\n",
    "# Lag Scatter Plot\n",
    "lag_plot(df_filtered['Sales'])\n",
    "plt.title('Lag Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and evaluate the model\n",
    "def build_and_evaluate_model(look_back, split_ratio):\n",
    "    # Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_aggregated['Sales_scaled'] = scaler.fit_transform(df_aggregated[['Sales']])\n",
    "\n",
    "    # Define look_forward\n",
    "    look_forward = 7  # Number of future time steps to forecast\n",
    "\n",
    "    # Data Sequencing\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "        X.append(df_aggregated['Sales_scaled'][i:i+look_back].values)\n",
    "        Y.append(df_aggregated['Sales_scaled'][i+look_back:i+look_back+look_forward].values)\n",
    "\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Model Building\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(look_forward))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Model Training\n",
    "    model.fit(X_train, Y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "    # Prediction\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse scaling\n",
    "    Y_test_inv = scaler.inverse_transform(Y_test)\n",
    "    Y_pred_inv = scaler.inverse_transform(Y_pred)\n",
    "\n",
    "    # Evaluation\n",
    "    mae = mean_absolute_error(Y_test_inv, Y_pred_inv)\n",
    "    mse = mean_squared_error(Y_test_inv, Y_pred_inv)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(Y_test_inv, Y_pred_inv)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Y_test_inv[0], label='Actual', marker='o')\n",
    "    plt.plot(Y_pred_inv[0], label='Predicted', marker='x')\n",
    "    plt.title('Actual vs Predicted Sales')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Forecasting future sales using last known sequence\n",
    "    last_known_seq = X[-1:]\n",
    "    last_known_seq = np.reshape(last_known_seq, (last_known_seq.shape[0], 1, last_known_seq.shape[1]))\n",
    "    future_sales_scaled = model.predict(last_known_seq)\n",
    "    future_sales = scaler.inverse_transform(future_sales_scaled)\n",
    "    \n",
    "    # Plotting the forecasted sales\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(future_sales[0], label='Forecasted Sales', marker='s')\n",
    "    plt.title('Forecasted Sales for Next ' + str(look_forward) + ' Days')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# Different scalers, look_backs, and split_ratios to try\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "# To store the best results\n",
    "best_mae = float('inf')\n",
    "best_config = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for look_back in look_backs:\n",
    "    for split_ratio in split_ratios:\n",
    "        mae, mse, rmse, r2 = build_and_evaluate_model(look_back, split_ratio)\n",
    "        print(f\"Look Back: {look_back}, Split Ratio: {1 - split_ratio}-{split_ratio*100}, MAE: {mae}, MSE: {mse}, RMSE: {rmse}, R2: {r2}\")\n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_config = {'look_back': look_back, 'split_ratio': split_ratio, 'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest, Linear Regression and Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define look_forward and look_back\n",
    "look_forward = 7  # Number of future time steps to forecast\n",
    "\n",
    "# Different algorithms to try\n",
    "algorithms = {'LinearRegression': LinearRegression(), \n",
    "              'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "              'SVR': SVR()}\n",
    "\n",
    "# Different look_backs and split_ratios to try\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "# To store the best results\n",
    "best_mae = float('inf')\n",
    "best_config = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for algo_name, algo in algorithms.items():\n",
    "    for look_back in look_backs:\n",
    "        for split_ratio in split_ratios:\n",
    "            # Data Sequencing\n",
    "            X, Y = [], []\n",
    "            for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "                X.append(df_aggregated['Sales'][i:i+look_back].values)\n",
    "                Y.append(df_aggregated['Sales'][i+look_back:i+look_back+look_forward].values)\n",
    "            X, Y = np.array(X), np.array(Y)\n",
    "            \n",
    "            # Train-Test Split\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_ratio, random_state=42)\n",
    "\n",
    "            # Model Building and Training\n",
    "            model = MultiOutputRegressor(algo)\n",
    "            model.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            # Evaluation\n",
    "            mae = mean_absolute_error(Y_test, Y_pred)\n",
    "            r2 = r2_score(Y_test, Y_pred, multioutput='variance_weighted')\n",
    "            \n",
    "            print(f\"Algorithm: {algo_name}, Look Back: {look_back}, Split Ratio: {1 - split_ratio}-{split_ratio*100}, MAE: {mae}, R2: {r2}\")\n",
    "            \n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_config = {'algorithm': algo_name, 'look_back': look_back, 'split_ratio': split_ratio, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def build_and_evaluate_cnn_model(df_aggregated, look_back, split_ratio):\n",
    "    # Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_aggregated['Sales_scaled'] = scaler.fit_transform(df_aggregated[['Sales']])\n",
    "    \n",
    "    # Define look_forward\n",
    "    look_forward = 7  # Number of future time steps to forecast\n",
    "    \n",
    "    # Data Sequencing\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "        X.append(df_aggregated['Sales_scaled'][i:i+look_back].values)\n",
    "        Y.append(df_aggregated['Sales_scaled'][i+look_back:i+look_back+look_forward].values)\n",
    "    \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_ratio, random_state=42)\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Model Building\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(look_forward))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Model Training\n",
    "    model.fit(X_train, Y_train, epochs=50, batch_size=1, verbose=1)\n",
    "    \n",
    "    # Prediction\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    Y_test_inv = scaler.inverse_transform(Y_test)\n",
    "    Y_pred_inv = scaler.inverse_transform(Y_pred)\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics = {}\n",
    "    metrics['MAE'] = mean_absolute_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['MSE'] = mean_squared_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['RMSE'] = math.sqrt(metrics['MSE'])\n",
    "    metrics['R2'] = r2_score(Y_test_inv, Y_pred_inv)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Y_test_inv[0], label='Actual', marker='o')\n",
    "    plt.plot(Y_pred_inv[0], label='Predicted', marker='x')\n",
    "    plt.title('Actual vs Predicted Sales')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Debugging\n",
    "    last_known_seq = X[-1:]\n",
    "    \n",
    "    last_known_seq = np.reshape(last_known_seq, (last_known_seq.shape[0], last_known_seq.shape[1], 1))\n",
    "    \n",
    "    future_sales_scaled = model.predict(last_known_seq)\n",
    "    future_sales = scaler.inverse_transform(future_sales_scaled)\n",
    "    \n",
    "    # Plotting the forecasted sales\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(future_sales[0], label='Forecasted Sales', marker='s')\n",
    "    plt.title('Forecasted Sales for Next ' + str(look_forward) + ' Days')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Different split_ratios to try\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "# To store the best results\n",
    "best_metrics = {'MAE': float('inf'), 'MSE': float('inf'), 'RMSE': float('inf'), 'R2': float('-inf')}\n",
    "best_config = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for look_back in look_backs:\n",
    "    for split_ratio in split_ratios:\n",
    "        metrics = build_and_evaluate_cnn_model(df_aggregated, look_back, split_ratio)\n",
    "        \n",
    "        print(f\"Look Back: {look_back}, Split Ratio: {1 - split_ratio}-{split_ratio*100}, Metrics: {metrics}\")\n",
    "        \n",
    "        if metrics['MAE'] < best_metrics['MAE'] and metrics['MSE'] < best_metrics['MSE'] and metrics['R2'] > best_metrics['R2']:\n",
    "            best_metrics = metrics\n",
    "            best_config = {'look_back': look_back, 'split_ratio': split_ratio}\n",
    "\n",
    "print(\"\\nBest Configuration and Metrics:\")\n",
    "print(\"Configuration:\", best_config)\n",
    "print(\"Metrics:\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN and Random Forest (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Sample function to build and evaluate a hybrid CNN-Random Forest model\n",
    "def build_and_evaluate_hybrid_model(look_back, split_ratio):\n",
    "    # Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_aggregated['Sales_scaled'] = scaler.fit_transform(df_aggregated[['Sales']])\n",
    "    \n",
    "    # Define look_forward\n",
    "    look_forward = 7  # Number of future time steps to forecast\n",
    "    \n",
    "    # Data Sequencing\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "        X.append(df_aggregated['Sales_scaled'][i:i+look_back].values)\n",
    "        Y.append(df_aggregated['Sales_scaled'][i+look_back:i+look_back+look_forward].values)\n",
    "    \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_ratio, random_state=42)\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # CNN Model Building\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(50, activation='relu'))\n",
    "    cnn_model.add(Dense(look_forward))  # This should match with 'look_forward'\n",
    "    cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # CNN Model Training\n",
    "    cnn_model.fit(X_train, Y_train, epochs=50, batch_size=1, verbose=1)\n",
    "    \n",
    "    # Feature extraction with CNN\n",
    "    feature_model = Sequential(cnn_model.layers[:-1])\n",
    "    X_train_transformed = feature_model.predict(X_train)\n",
    "    X_test_transformed = feature_model.predict(X_test)\n",
    "    \n",
    "    # Random Forest Model Building\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_transformed, Y_train)\n",
    "    \n",
    "    # Prediction with Random Forest\n",
    "    Y_pred = rf_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    Y_test_inv = scaler.inverse_transform(Y_test)\n",
    "    Y_pred_inv = scaler.inverse_transform(Y_pred)\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics = {}\n",
    "    metrics['MAE'] = mean_absolute_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['MSE'] = mean_squared_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['RMSE'] = math.sqrt(metrics['MSE'])\n",
    "    metrics['R2'] = r2_score(Y_test_inv, Y_pred_inv)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Different split_ratios to try\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "# To store the best results\n",
    "best_metrics = {'MAE': float('inf'), 'MSE': float('inf'), 'RMSE': float('inf'), 'R2': float('-inf')}\n",
    "best_config = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for look_back in look_backs:\n",
    "    for split_ratio in split_ratios:\n",
    "        metrics = build_and_evaluate_hybrid_model(look_back, split_ratio)\n",
    "        \n",
    "        print(f\"Look Back: {look_back}, Split Ratio: {1 - split_ratio}-{split_ratio*100}, Metrics: {metrics}\")\n",
    "        \n",
    "        # Update best metrics and configuration if needed\n",
    "        if metrics['MAE'] < best_metrics['MAE'] and metrics['MSE'] < best_metrics['MSE'] and metrics['R2'] > best_metrics['R2']:\n",
    "            best_metrics = metrics\n",
    "            best_config = {'look_back': look_back, 'split_ratio': split_ratio}\n",
    "\n",
    "print(\"\\nBest Configuration and Metrics:\")\n",
    "print(\"Configuration:\", best_config)\n",
    "print(\"Metrics:\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and Random Forest (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Sample function to build and evaluate a hybrid LSTM-Random Forest model\n",
    "def build_and_evaluate_hybrid_model(look_back, split_ratio):\n",
    "    # Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_aggregated['Sales_scaled'] = scaler.fit_transform(df_aggregated[['Sales']])\n",
    "    \n",
    "    # Define look_forward\n",
    "    look_forward = 7  # Number of future time steps to forecast\n",
    "    \n",
    "    # Data Sequencing\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "        X.append(df_aggregated['Sales_scaled'][i:i+look_back].values)\n",
    "        Y.append(df_aggregated['Sales_scaled'][i+look_back:i+look_back+look_forward].values)\n",
    "    \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_ratio, random_state=42)\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # LSTM Model Building\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    lstm_model.add(Dense(look_forward))\n",
    "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # LSTM Model Training\n",
    "    lstm_model.fit(X_train, Y_train, epochs=50, batch_size=1, verbose=1)\n",
    "    \n",
    "    # Feature extraction with LSTM\n",
    "    feature_model = Sequential(lstm_model.layers[:-1])\n",
    "    X_train_transformed = feature_model.predict(X_train)\n",
    "    X_test_transformed = feature_model.predict(X_test)\n",
    "    \n",
    "    # Random Forest Model Building\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_transformed, Y_train)\n",
    "    \n",
    "    # Prediction with Random Forest\n",
    "    Y_pred = rf_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    Y_test_inv = scaler.inverse_transform(Y_test)\n",
    "    Y_pred_inv = scaler.inverse_transform(Y_pred)\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics = {}\n",
    "    metrics['MAE'] = mean_absolute_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['MSE'] = mean_squared_error(Y_test_inv, Y_pred_inv)\n",
    "    metrics['RMSE'] = math.sqrt(metrics['MSE'])\n",
    "    metrics['R2'] = r2_score(Y_test_inv, Y_pred_inv)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Different split_ratios to try\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "# To store the best results\n",
    "best_metrics = {'MAE': float('inf'), 'MSE': float('inf'), 'RMSE': float('inf'), 'R2': float('-inf')}\n",
    "best_config = {}\n",
    "\n",
    "# Loop through all combinations\n",
    "for look_back in look_backs:\n",
    "    for split_ratio in split_ratios:\n",
    "        metrics = build_and_evaluate_hybrid_model(look_back, split_ratio)\n",
    "        \n",
    "        print(f\"Look Back: {look_back}, Split Ratio: {1 - split_ratio}-{split_ratio*100}, Metrics: {metrics}\")\n",
    "        \n",
    "        # Update best metrics and configuration if needed\n",
    "        if metrics['MAE'] < best_metrics['MAE'] and metrics['MSE'] < best_metrics['MSE'] and metrics['R2'] > best_metrics['R2']:\n",
    "            best_metrics = metrics\n",
    "            best_config = {'look_back': look_back, 'split_ratio': split_ratio}\n",
    "\n",
    "print(\"\\nBest Configuration and Metrics:\")\n",
    "print(\"Configuration:\", best_config)\n",
    "print(\"Metrics:\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def build_and_evaluate_gru_model(df_aggregated, look_back, split_ratio):\n",
    "    # Feature Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df_aggregated['Sales_scaled'] = scaler.fit_transform(df_aggregated[['Sales']])\n",
    "    \n",
    "    # Define look_forward\n",
    "    look_forward = 7\n",
    "    \n",
    "    # Data Sequencing\n",
    "    X, Y = [], []\n",
    "    for i in range(len(df_aggregated) - look_back - look_forward + 1):\n",
    "        X.append(df_aggregated['Sales_scaled'][i:i+look_back])\n",
    "        Y.append(df_aggregated['Sales_scaled'][i+look_back:i+look_back+look_forward])\n",
    "        \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_ratio, random_state=42)\n",
    "    \n",
    "    # Reshape input\n",
    "    X_train = X_train.reshape((X_train.shape[0], look_back, 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], look_back, 1))\n",
    "    \n",
    "    # Build GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, activation='relu', input_shape=(look_back, 1)))\n",
    "    model.add(Dense(look_forward))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, epochs=50, batch_size=1, verbose=1)\n",
    "    \n",
    "    # Predict\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform\n",
    "    Y_pred_inv = scaler.inverse_transform(Y_pred)\n",
    "    Y_test_inv = scaler.inverse_transform(Y_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = {\n",
    "        'MAE': mean_absolute_error(Y_test_inv, Y_pred_inv),\n",
    "        'MSE': mean_squared_error(Y_test_inv, Y_pred_inv),\n",
    "        'RMSE': math.sqrt(mean_squared_error(Y_test_inv, Y_pred_inv)),\n",
    "        'R2': r2_score(Y_test_inv, Y_pred_inv)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Y_test_inv[0], label='Actual', marker='o')\n",
    "    plt.plot(Y_pred_inv[0], label='Predicted', marker='x')\n",
    "    plt.title('Actual vs Predicted Sales')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Debugging\n",
    "    last_known_seq = X[-1:]\n",
    "    \n",
    "    last_known_seq = np.reshape(last_known_seq, (last_known_seq.shape[0], last_known_seq.shape[1], 1))\n",
    "    \n",
    "    future_sales_scaled = model.predict(last_known_seq)\n",
    "    future_sales = scaler.inverse_transform(future_sales_scaled)\n",
    "    \n",
    "    # Plotting the forecasted sales\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(future_sales[0], label='Forecasted Sales', marker='s')\n",
    "    plt.title('Forecasted Sales for Next ' + str(look_forward) + ' Days')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.xlabel('Future Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "look_backs = [7]\n",
    "split_ratios = [0.4, 0.3, 0.2]\n",
    "\n",
    "best_metrics = {'MAE': float('inf'), 'MSE': float('inf'), 'RMSE': float('inf'), 'R2': -1}\n",
    "\n",
    "for look_back in look_backs:\n",
    "    for split_ratio in split_ratios:\n",
    "        metrics = build_and_evaluate_gru_model(df_aggregated, look_back, split_ratio)\n",
    "        print(f\"Look Back: {look_back}, Split Ratio: {split_ratio}, Metrics: {metrics}\")\n",
    "        \n",
    "        if metrics['R2'] > best_metrics['R2']:\n",
    "            best_metrics = metrics\n",
    "            best_config = {'look_back': look_back, 'split_ratio': split_ratio}\n",
    "\n",
    "print(\"\\nBest Configuration and Metrics:\")\n",
    "print(\"Configuration:\", best_config)\n",
    "print(\"Metrics:\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
